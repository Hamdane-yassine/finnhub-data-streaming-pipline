{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, window\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"market\"\n",
    "KAFKA_SERVER = \"localhost\"\n",
    "KAFKA_PORT = 9094\n",
    "KAFKA_CLIENT_VERSION = \"3.7.0\"\n",
    "\n",
    "SCALA_VERSION = '2.12'\n",
    "SPARK_VERSION = '3.5.1'\n",
    "SPARK_MASTER = \"local[*]\"\n",
    "SHUFFLE_PARTITIONS = 5\n",
    "\n",
    "CASSANDRA_SERVER = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "\n",
    "APP_NAME = \"BigDataStreaming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/04/19 20:18:00 WARN Utils: Your hostname, furyPIRATE resolves to a loopback address: 127.0.1.1; using 172.29.240.184 instead (on interface eth0)\n",
      "24/04/19 20:18:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/furypirate/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/furypirate/.ivy2/cache\n",
      "The jars for the packages stored in: /home/furypirate/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector-assembly_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-be6c327d-e4de-4df4-9f18-e927c064c12a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.7.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-6 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-assembly_2.12;3.5.0 in central\n",
      ":: resolution report :: resolve 645ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.spark#spark-cassandra-connector-assembly_2.12;3.5.0 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.5.5-6 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.7.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.7.0] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   3   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-be6c327d-e4de-4df4-9f18-e927c064c12a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/10ms)\n",
      "24/04/19 20:18:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.29.240.184:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigDataStreaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c20ab47f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION}',\n",
    "    f'org.apache.kafka:kafka-clients:{KAFKA_CLIENT_VERSION}',\n",
    "    f'org.apache.spark:spark-avro_{SCALA_VERSION}:{SPARK_VERSION}',\n",
    "    f\"com.datastax.spark:spark-cassandra-connector-assembly_{SCALA_VERSION}:3.5.0\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "   .master(SPARK_MASTER)\\\n",
    "   .appName(APP_NAME)\\\n",
    "   .config(\"spark.sql.shuffle.partitions\", f'{SHUFFLE_PARTITIONS}')\\\n",
    "   .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "   .config(\"spark.cassandra.connection.host\",f\"{CASSANDRA_SERVER}:{CASSANDRA_PORT}\")\\\n",
    "   .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "market_stream: DataFrame = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{KAFKA_SERVER}:{KAFKA_PORT}\")\\\n",
    "    .option(\"subscribe\", \"market\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"failOnDataLoss\", \"false\")\\\n",
    "    .load()\n",
    "market_stream.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe transformation query to extract AVRO data into corresponding fields, and format the timestamp correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: double (nullable = false)\n",
      " |-- symbol: string (nullable = false)\n",
      " |-- volume: double (nullable = false)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/19 20:18:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f6c20c29400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avro_schema = open(\"../finnhub/trade.avsc\", \"r\").read()\n",
    "\n",
    "trades_stream = market_stream\\\n",
    "    .withColumn(\"trade_data\", from_avro(\"value\", avro_schema))\\\n",
    "    .select(\"trade_data\", \"offset\")\\\n",
    "    .select(\"trade_data.*\", \"offset\")\\\n",
    "    .select(explode(\"data\"),\"type\", \"offset\")\\\n",
    "    .select(\"col.*\", \"offset\")\\\n",
    "    .selectExpr(\"p as price\", \"s as symbol\", \"v as volume\", \"cast(cast(t as double) / 1000 as timestamp) as event_time\", \"offset\") \n",
    "\n",
    "trades_stream.printSchema()\n",
    "\n",
    "trades_stream.writeStream\\\n",
    "    .queryName(\"trades\")\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"checkpointLocation\", '/tmp/checkpoint/trades/') \\\n",
    "    .options(table=\"trades\",keyspace=\"market\") \\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query for the minute_trades table to calculate the count and average price of trades on a window of 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_trades_query = trades_stream\\\n",
    "    .groupby(window(\"event_time\", \"1 day\"))\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"minute_trades\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\").start()\n",
    "    # .agg({\"*\" : \"count\", \"price\" : \"avg\", \"offset\" : \"max\"})\\\n",
    "    # .withColumnsRenamed({\"avg(price)\":\"avg_price\", \"count(1)\":\"total\", \"max(offset)\":\"id\"})\\\n",
    "    # .selectExpr(\"id\", \"avg_price\", \"total\", \"window.end as event_time\", \"window.start as start\")\\\n",
    "    \n",
    "    \n",
    "\n",
    "    # .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    # .option(\"checkpointLocation\", '/tmp/checkpoint/minute_trades/') \\\n",
    "    # .option(\"confirm.truncate\", \"true\")\\\n",
    "    # .options(table = \"minute_trades\", keyspace = \"market\") \\\n",
    "    # .outputMode(\"Append\")\\\n",
    "    # .start()\n",
    "\n",
    "    # .queryName(\"minute_trades\")\\\n",
    "    # .format(\"memory\")\\\n",
    "    # .outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    spark.sql(\"SELECT count(*) FROM trades\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
